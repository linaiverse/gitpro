import pandas as pd
from pymongo import MongoClient
from typing import List, Optional, Dict, Any
import time
import argparse
import json

start = time.time()

# åˆ›å»ºArgumentParserå¯¹è±¡
parser = argparse.ArgumentParser(description='Retrieve data from the database')

# ä¿®æ”¹å‚æ•°å®šä¹‰ - ä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹ï¼Œç¨åè§£æä¸ºåˆ—è¡¨
parser.add_argument('collections', type=str, help='collections of your want (as JSON list)')
parser.add_argument('fields', type=str, help='fields of your want (as JSON list)')
parser.add_argument('csvname', type=str, help='name of your dumped file')
parser.add_argument('--filter', type=str, default='{}', help='filter condition (as JSON object)')

# è§£æå‚æ•°
args = parser.parse_args()

def batch_merge_collections(
    db_name: str,
    collection_names: List[str],
    batch_size: int = 5000,
    fields: Optional[List[str]] = None,
    filter_query: Optional[Dict[str, Any]] = None  
) -> List[dict]:
    # è¿æ¥ MongoDB
    client = MongoClient(
        host='10.128.11.182',
        port=27017,
        username='senbatest',
        password='mdGBp7ck',
        authSource='admin',
        authMechanism='SCRAM-SHA-1'
    )
    
    db = client[db_name]
    merged_docs = []
    # è·å–å½“å‰æ•°æ®åº“ä¸­æ‰€æœ‰å­˜åœ¨çš„é›†åˆåç§°ï¼ˆç”¨äºåç»­æ£€æŸ¥ï¼‰
    existing_collections = db.list_collection_names()
    
    for col_name in collection_names:
        # æ ¸å¿ƒä¿®æ”¹ï¼šæ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è·³è¿‡
        if col_name not in existing_collections:
            print(f"âš ï¸  Collection '{col_name}' does NOT exist, skipping...")
            continue
        
        print(f"âœ…  Processing collection: {col_name}")
        
        # æ„å»ºæŠ•å½±ï¼ˆå­—æ®µé€‰æ‹©è§„åˆ™ï¼‰
        projection = {"_id": 0}  # é»˜è®¤ä¸åŒ…å«_idï¼ˆé¿å…MongoDBé»˜è®¤çš„ObjectIdå¹²æ‰°ï¼‰
        if fields is not None and len(fields) > 0:
            for field in fields:
                projection[field] = 1  # ä»…ä¿ç•™æŒ‡å®šå­—æ®µ

        # ä½¿ç”¨æŠ•å½±+è¿‡æ»¤æ¡ä»¶æŸ¥è¯¢
        cursor = db[col_name].find(
            filter=filter_query if (filter_query and filter_query != {}) else {},
            projection=projection,
            batch_size=batch_size
        )
        
        # æµå¼å¤„ç†æ–‡æ¡£ï¼ˆé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§é‡æ•°æ®åˆ°å†…å­˜ï¼‰
        count = 0
        for doc in cursor:
            merged_docs.append(doc)
            count += 1
        
        print(f"ğŸ“Š  Retrieved {count} documents from '{col_name}'")

    # å…³é—­MongoDBè¿æ¥ï¼ˆé¿å…èµ„æºæ³„æ¼ï¼‰
    client.close()
    return merged_docs

# è§£æå‚æ•°ä¸ºå®é™…åˆ—è¡¨/å­—å…¸ï¼ˆå¤„ç†å•å¼•å·è½¬åŒå¼•å·ï¼Œé€‚é…JSONæ ¼å¼ï¼‰
try:
    # å°†å‘½ä»¤è¡Œä¼ å…¥çš„å•å¼•å·æ›¿æ¢ä¸ºåŒå¼•å·ï¼Œç¡®ä¿JSONè§£ææˆåŠŸ
    collections = json.loads(args.collections.replace("'", '"'))
    fields = json.loads(args.fields.replace("'", '"'))
    filter_query = json.loads(args.filter.replace("'", '"'))
    # æ ¡éªŒcollectionsæ˜¯å¦ä¸ºåˆ—è¡¨ï¼ˆé¿å…å‚æ•°æ ¼å¼é”™è¯¯ï¼‰
    if not isinstance(collections, list):
        raise ValueError("'collections' must be a JSON list (e.g., [\"col1\",\"col2\"])")
    if not isinstance(fields, list):
        raise ValueError("'fields' must be a JSON list (e.g., [\"field1\",\"field2\"])")
except json.JSONDecodeError as e:
    print(f"âŒ  Error parsing arguments: Invalid JSON format - {e}")
    print("ğŸ“  Example usage: python3 getMongo4.0.py '[\"col1\",\"col2\"]' '[\"field1\",\"field2\"]' 'output.csv'")
    exit(1)
except ValueError as e:
    print(f"âŒ  Argument format error: {e}")
    exit(1)

# æ‰“å°å‚æ•°ç¡®è®¤ä¿¡æ¯
print(f"\nğŸ”§  Configuration Summary:")
print(f"    Collections to process: {collections}")
print(f"    Fields to retrieve: {fields}")
print(f"    Filter condition: {filter_query if filter_query != {} else 'No filter'}")
print(f"    Output CSV path: {args.csvname}\n")

# æ‰§è¡ŒMongoDBæŸ¥è¯¢
result = batch_merge_collections(
    db_name="senba_gps",
    collection_names=collections,
    batch_size=50000,  # å¤§æ‰¹æ¬¡å‡å°‘I/Oæ¬¡æ•°ï¼Œæå‡æ•ˆç‡
    fields=fields,
    filter_query=filter_query
)

# å¤„ç†æŸ¥è¯¢ç»“æœå¹¶ä¿å­˜åˆ°CSV
if result and len(result) > 0:
    df = pd.DataFrame(result)
    print(f"\nğŸ“ˆ  Total retrieved records: {len(df)}")
    print(f"ğŸ”  Sample data (first 5 rows):")
    print(df.head(5))
    
    # ä¿å­˜CSVï¼ˆindex=Falseé¿å…å¤šä½™çš„ç´¢å¼•åˆ—ï¼‰
    df.to_csv(args.csvname, index=False, encoding='utf-8-sig')  # utf-8-sigæ”¯æŒä¸­æ–‡æ˜¾ç¤º
    print(f"\nğŸ’¾  Data successfully saved to: {args.csvname}")
else:
    print(f"\nâš ï¸  No valid data retrieved from MongoDB (all collections may be empty or non-existent)")

# è®¡ç®—å¹¶æ‰“å°æ€»æ‰§è¡Œæ—¶é—´
end = time.time()
print(f"\nâ±ï¸  Total execution time: {round(end - start, 2)} seconds")